{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-26T16:31:28.475878Z",
     "start_time": "2025-01-26T16:31:28.457388Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "DOES NOT WORK : print is good, but the code after is not handled (BLUE, RED, etc) !!\n",
    "\"\"\"\n",
    "import common_immunogit"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure : Done.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T16:40:20.786482Z",
     "start_time": "2025-01-26T16:40:20.776611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Code is able to make a url-compatible query, offset and number of results are relevant parameters for a later part if we get >100 results !\n",
    "\"\"\"\n",
    "\n",
    "def create_query(domain=\"biomodels\", offset=0, num_results=100):\n",
    "    query_parts_full = {\n",
    "        'mode': '*:*',\n",
    "        'species': 'TAXONOMY:9606',\n",
    "        'curation_status': 'curationstatus:\"Manually curated\"',\n",
    "        'formats': 'modelformat:\"SBML\"',\n",
    "        'kw': 'submitter_keywords:\"Immuno-oncology\"'\n",
    "    }\n",
    "\n",
    "    # Human-like query / Browsing query\n",
    "    query_parts = [value for value in query_parts_full.values() if value]\n",
    "    query = \" AND \".join(query_parts)\n",
    "\n",
    "    # URL-compatible query\n",
    "    query_for_url = query.replace(\" \", \"%20\").replace(\":\", \"%3A\").replace('\"', \"%22\")\n",
    "    url = f\"https://www.ebi.ac.uk/biomodels/search?query={query_for_url}&domain={domain}&offset={offset}&numResults={num_results}\"\n",
    "\n",
    "    return query, url\n",
    "\n",
    "query, url = create_query(domain=\"biomodels\", offset=0, num_results=100)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"URL: {url}\")"
   ],
   "id": "642cc4f5a6032bbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: *:* AND TAXONOMY:9606 AND curationstatus:\"Manually curated\" AND modelformat:\"SBML\" AND submitter_keywords:\"Immuno-oncology\"\n",
      "URL: https://www.ebi.ac.uk/biomodels/search?query=*%3A*%20AND%20TAXONOMY%3A9606%20AND%20curationstatus%3A%22Manually%20curated%22%20AND%20modelformat%3A%22SBML%22%20AND%20submitter_keywords%3A%22Immuno-oncology%22&domain=biomodels&offset=0&numResults=100\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:10:13.086959Z",
     "start_time": "2025-01-26T17:10:10.771481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "THE SOURCE CODE IS RETRIEVED THIS WAY AND WORKS WITH JUPYTER NOTEBOOKS.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def scrape_page():\n",
    "    query, url = create_query(domain=\"biomodels\", offset=0, num_results=100)\n",
    "    output_file = \"webpage_source.html\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)  # headless=False : see the browser\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "        page_source = await page.content()\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(page_source)\n",
    "\n",
    "        print(f\"Source code saved to {output_file}, originates from {url}\")\n",
    "        await browser.close()\n",
    "\n",
    "asyncio.run(scrape_page())"
   ],
   "id": "4374f005e5d439dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source code saved to webpage_source.html, originates from https://www.ebi.ac.uk/biomodels/search?query=*%3A*%20AND%20TAXONOMY%3A9606%20AND%20curationstatus%3A%22Manually%20curated%22%20AND%20modelformat%3A%22SBML%22%20AND%20submitter_keywords%3A%22Immuno-oncology%22&domain=biomodels&offset=0&numResults=100\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:10:33.758497Z",
     "start_time": "2025-01-26T17:10:33.751657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "HTML file from the query + regex to make a list of relevant IDs.\n",
    "WORKS FINE OF MANUALLY RETRIEVED SOURCE CODE.\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def extract_ids(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "        pattern = r\"(BIOMD\\d{10}|MODEL\\d{10})\"\n",
    "        ids = set()\n",
    "\n",
    "        for element in soup.find_all(string=True):\n",
    "            content = element.strip()\n",
    "            matches = re.findall(pattern, content)\n",
    "            ids.update(matches)\n",
    "\n",
    "        ids = sorted(ids)\n",
    "        return ids\n",
    "    except:\n",
    "        pass"
   ],
   "id": "37d3348430de8223",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:11:08.644960Z",
     "start_time": "2025-01-26T17:11:08.602781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "DEMO : I tested both generated URL and manual URL, results (ids == ids2 returned TRUE) are the same !\n",
    "\"\"\"\n",
    "BLUE = '\\033[94m'\n",
    "RESET = '\\033[0m'\n",
    "\n",
    "file_path = \"/Users/guillaume.souede/PycharmProjects/immunogit/src/webpage_source.html\"\n",
    "ids = extract_ids(file_path)\n",
    "print(\"IDs:\", ids)\n",
    "print(f\"{BLUE}NUMBER OF QUERY MATCHES : {len(ids)}.{RESET}\")"
   ],
   "id": "c8b70ab4c1b89e88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs: ['BIOMD0000000741', 'BIOMD0000000742', 'BIOMD0000000743', 'BIOMD0000000744', 'BIOMD0000000746', 'BIOMD0000000748', 'BIOMD0000000749', 'BIOMD0000000751', 'BIOMD0000000752', 'BIOMD0000000753', 'BIOMD0000000754', 'BIOMD0000000756', 'BIOMD0000000757', 'BIOMD0000000758', 'BIOMD0000000759', 'BIOMD0000000760', 'BIOMD0000000764', 'BIOMD0000000766', 'BIOMD0000000767', 'BIOMD0000000768', 'BIOMD0000000769', 'BIOMD0000000770', 'BIOMD0000000778', 'BIOMD0000000780', 'BIOMD0000000781', 'BIOMD0000000782', 'BIOMD0000000791', 'BIOMD0000000798', 'BIOMD0000000801', 'BIOMD0000000802', 'BIOMD0000000812', 'BIOMD0000000813', 'BIOMD0000000877', 'BIOMD0000000879', 'BIOMD0000000880', 'BIOMD0000000885', 'BIOMD0000000886', 'BIOMD0000000888', 'BIOMD0000000891', 'BIOMD0000000894', 'BIOMD0000000900', 'BIOMD0000000904', 'BIOMD0000000908', 'BIOMD0000000909', 'BIOMD0000000910', 'BIOMD0000000911', 'BIOMD0000000912', 'BIOMD0000000913', 'BIOMD0000000919', 'BIOMD0000000921', 'BIOMD0000000926', 'BIOMD0000001011', 'BIOMD0000001012', 'BIOMD0000001013', 'BIOMD0000001014', 'BIOMD0000001022', 'BIOMD0000001023', 'BIOMD0000001024', 'BIOMD0000001025', 'BIOMD0000001031', 'BIOMD0000001032', 'BIOMD0000001033', 'BIOMD0000001034', 'BIOMD0000001035', 'BIOMD0000001036', 'BIOMD0000001041', 'BIOMD0000001042', 'BIOMD0000001043']\n",
      "\u001B[94mNUMBER OF QUERY MATCHES : 68.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-26T17:57:03.564340Z",
     "start_time": "2025-01-26T17:57:00.534008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import nest_asyncio\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\"\"\"\n",
    "ABOVE CODE SHOULD GO IN common_immunogit.py\n",
    "\"\"\"\n",
    "def create_query(domain=\"biomodels\", offset=0, num_results=100):\n",
    "    query_parts_full = {\n",
    "        'mode': '*:*',\n",
    "        'species': 'TAXONOMY:9606',\n",
    "        'curation_status': 'curationstatus:\"Manually curated\"',\n",
    "        'formats': 'modelformat:\"SBML\"',\n",
    "        'kw': 'submitter_keywords:\"Immuno-oncology\"'\n",
    "    }\n",
    "\n",
    "    query_parts = [value for value in query_parts_full.values() if value]\n",
    "    query = \" AND \".join(query_parts)\n",
    "\n",
    "    query_for_url = query.replace(\" \", \"%20\").replace(\":\", \"%3A\").replace('\"', \"%22\")\n",
    "    url = f\"https://www.ebi.ac.uk/biomodels/search?query={query_for_url}&domain={domain}&offset={offset}&numResults={num_results}\"\n",
    "\n",
    "    return query, url\n",
    "\n",
    "def extract_ids(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "        pattern = r\"(BIOMD\\d{10}|MODEL\\d{10})\"\n",
    "        ids = set()\n",
    "\n",
    "        for element in soup.find_all(string=True):\n",
    "            content = element.strip()\n",
    "            matches = re.findall(pattern, content)\n",
    "            ids.update(matches)\n",
    "\n",
    "        ids = sorted(ids)\n",
    "        return ids\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return []\n",
    "\n",
    "async def scrape_page(offset=0, num_results=100):\n",
    "    query, url = create_query(domain=\"biomodels\", offset=offset, num_results=num_results)\n",
    "    output_file = \"webpage_source.html\"\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "        page_source = await page.content()\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(page_source)\n",
    "\n",
    "        print(f\"{BLUE}File saved : {output_file}{RESET}\")\n",
    "        await browser.close()\n",
    "\n",
    "    ids = extract_ids(output_file)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "        print(f\"{BLUE}File deleted: {output_file}{RESET}\")\n",
    "    return ids\n",
    "\n",
    "async def main():\n",
    "    offset = 0\n",
    "    num_results = 100\n",
    "    ids_agreg = []\n",
    "\n",
    "    while True:\n",
    "        print(f\"{BLUE}Current offset : {offset}{RESET}\")\n",
    "        ids = await scrape_page(offset, num_results)\n",
    "\n",
    "        if not ids:\n",
    "            print(f\"{BLUE}Stopping : no more IDs.{RESET}\")\n",
    "            break\n",
    "\n",
    "        ids_agreg.extend(ids)\n",
    "        print(f\"{BLUE}Stopping : collected {len(ids)} IDs, below 100 limit.{RESET}\")\n",
    "        print(f\"{BLUE}Totaling {len(ids_agreg)} IDs.{RESET}\")\n",
    "\n",
    "        if len(ids) < num_results:\n",
    "            break\n",
    "\n",
    "        offset += num_results\n",
    "\n",
    "    print(f\"Collected IDs : {ids_agreg}\")\n",
    "    print(f\"{BLUE}Number of Collected IDs : {len(ids_agreg)}{RESET}\")\n",
    "\n",
    "asyncio.run(main())"
   ],
   "id": "3b44f9687413b18e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[94mCurrent offset : 0\u001B[0m\n",
      "\u001B[94mFile saved : webpage_source.html\u001B[0m\n",
      "\u001B[94mFile deleted: webpage_source.html\u001B[0m\n",
      "\u001B[94mStopping : collected 68 IDs, below 100 limit.\u001B[0m\n",
      "\u001B[94mTotaling 68 IDs.\u001B[0m\n",
      "Collected IDs : ['BIOMD0000000741', 'BIOMD0000000742', 'BIOMD0000000743', 'BIOMD0000000744', 'BIOMD0000000746', 'BIOMD0000000748', 'BIOMD0000000749', 'BIOMD0000000751', 'BIOMD0000000752', 'BIOMD0000000753', 'BIOMD0000000754', 'BIOMD0000000756', 'BIOMD0000000757', 'BIOMD0000000758', 'BIOMD0000000759', 'BIOMD0000000760', 'BIOMD0000000764', 'BIOMD0000000766', 'BIOMD0000000767', 'BIOMD0000000768', 'BIOMD0000000769', 'BIOMD0000000770', 'BIOMD0000000778', 'BIOMD0000000780', 'BIOMD0000000781', 'BIOMD0000000782', 'BIOMD0000000791', 'BIOMD0000000798', 'BIOMD0000000801', 'BIOMD0000000802', 'BIOMD0000000812', 'BIOMD0000000813', 'BIOMD0000000877', 'BIOMD0000000879', 'BIOMD0000000880', 'BIOMD0000000885', 'BIOMD0000000886', 'BIOMD0000000888', 'BIOMD0000000891', 'BIOMD0000000894', 'BIOMD0000000900', 'BIOMD0000000904', 'BIOMD0000000908', 'BIOMD0000000909', 'BIOMD0000000910', 'BIOMD0000000911', 'BIOMD0000000912', 'BIOMD0000000913', 'BIOMD0000000919', 'BIOMD0000000921', 'BIOMD0000000926', 'BIOMD0000001011', 'BIOMD0000001012', 'BIOMD0000001013', 'BIOMD0000001014', 'BIOMD0000001022', 'BIOMD0000001023', 'BIOMD0000001024', 'BIOMD0000001025', 'BIOMD0000001031', 'BIOMD0000001032', 'BIOMD0000001033', 'BIOMD0000001034', 'BIOMD0000001035', 'BIOMD0000001036', 'BIOMD0000001041', 'BIOMD0000001042', 'BIOMD0000001043']\n",
      "\u001B[94mNumber of Collected IDs : 68\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1c4747bb8344daba"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
