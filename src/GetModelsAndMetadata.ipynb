{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T19:43:03.234573Z",
     "start_time": "2025-01-28T19:43:03.231791Z"
    }
   },
   "cell_type": "code",
   "source": "from common_immunogit import *",
   "id": "51638e82aeeabfdb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T19:43:03.253055Z",
     "start_time": "2025-01-28T19:43:03.248884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "Web Scraping Approach\n",
    "\"\"\"\n",
    "def create_query(domain=\"biomodels\", offset=0, num_results=100):\n",
    "    query_parts_full = {\n",
    "        'mode': '*:*',\n",
    "        'species': 'TAXONOMY:9606',\n",
    "        'curation_status': 'curationstatus:\"Manually curated\"',\n",
    "        'formats': 'modelformat:\"SBML\"',\n",
    "        'kw': 'submitter_keywords:\"Immuno-oncology\"'\n",
    "    }\n",
    "\n",
    "    query_parts = [value for value in query_parts_full.values() if value]\n",
    "    query = \" AND \".join(query_parts)\n",
    "\n",
    "    query_for_url = query.replace(\" \", \"%20\").replace(\":\", \"%3A\").replace('\"', \"%22\")\n",
    "    url = f\"https://www.ebi.ac.uk/biomodels/search?query={query_for_url}&domain={domain}&offset={offset}&numResults={num_results}\"\n",
    "\n",
    "    return query, url"
   ],
   "id": "4828cdf903d1f66c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T19:43:08.681924Z",
     "start_time": "2025-01-28T19:43:03.256955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_ids(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            soup = BeautifulSoup(file, \"html.parser\")\n",
    "\n",
    "        pattern = r\"(BIOMD\\d{10}|MODEL\\d{10})\"\n",
    "        ids = set()\n",
    "\n",
    "        for element in soup.find_all(string=True):\n",
    "            content = element.strip()\n",
    "            matches = re.findall(pattern, content)\n",
    "            ids.update(matches)\n",
    "\n",
    "        ids = sorted(ids)\n",
    "        return ids\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "        return []\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def scrape_page(offset=0, num_results=100):\n",
    "    query, url = create_query(domain=\"biomodels\", offset=offset, num_results=num_results)\n",
    "    output_file = \"webpage_source.html\"\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(url)\n",
    "        await page.wait_for_load_state(\"networkidle\")\n",
    "        page_source = await page.content()\n",
    "\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(page_source)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    ids = extract_ids(output_file)\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "\n",
    "    return ids\n",
    "\n",
    "async def main():\n",
    "    offset = 0\n",
    "    num_results = 100\n",
    "    ids_agreg = []\n",
    "\n",
    "    while True:\n",
    "        ids = await scrape_page(offset, num_results)\n",
    "\n",
    "        if not ids:\n",
    "            break\n",
    "\n",
    "        ids_agreg.extend(ids)\n",
    "\n",
    "        if len(ids) < num_results:\n",
    "            break\n",
    "\n",
    "        offset += num_results\n",
    "\n",
    "    ids_agreg_fp = os.path.join(root_path, \"tmp\", \"ids_agreg.json\")\n",
    "\n",
    "    with open(ids_agreg_fp, \"w\") as f:\n",
    "        json.dump(ids_agreg, f)\n",
    "\n",
    "    print(f\"Collected IDs : {ids_agreg}\")\n",
    "    print(f\"Number of Collected IDs : {len(ids_agreg)}\")\n",
    "\n",
    "    return ids_agreg\n",
    "\n",
    "collected_ids = asyncio.run(main())"
   ],
   "id": "3b44f9687413b18e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected IDs : ['BIOMD0000000741', 'BIOMD0000000742', 'BIOMD0000000743', 'BIOMD0000000744', 'BIOMD0000000746', 'BIOMD0000000748', 'BIOMD0000000749', 'BIOMD0000000751', 'BIOMD0000000752', 'BIOMD0000000753', 'BIOMD0000000754', 'BIOMD0000000756', 'BIOMD0000000757', 'BIOMD0000000758', 'BIOMD0000000759', 'BIOMD0000000760', 'BIOMD0000000764', 'BIOMD0000000766', 'BIOMD0000000767', 'BIOMD0000000768', 'BIOMD0000000769', 'BIOMD0000000770', 'BIOMD0000000778', 'BIOMD0000000780', 'BIOMD0000000781', 'BIOMD0000000782', 'BIOMD0000000791', 'BIOMD0000000798', 'BIOMD0000000801', 'BIOMD0000000802', 'BIOMD0000000812', 'BIOMD0000000813', 'BIOMD0000000877', 'BIOMD0000000879', 'BIOMD0000000880', 'BIOMD0000000885', 'BIOMD0000000886', 'BIOMD0000000888', 'BIOMD0000000891', 'BIOMD0000000894', 'BIOMD0000000900', 'BIOMD0000000904', 'BIOMD0000000908', 'BIOMD0000000909', 'BIOMD0000000910', 'BIOMD0000000911', 'BIOMD0000000912', 'BIOMD0000000913', 'BIOMD0000000919', 'BIOMD0000000921', 'BIOMD0000000926', 'BIOMD0000001011', 'BIOMD0000001012', 'BIOMD0000001013', 'BIOMD0000001014', 'BIOMD0000001022', 'BIOMD0000001023', 'BIOMD0000001024', 'BIOMD0000001025', 'BIOMD0000001031', 'BIOMD0000001032', 'BIOMD0000001033', 'BIOMD0000001034', 'BIOMD0000001035', 'BIOMD0000001036', 'BIOMD0000001041', 'BIOMD0000001042', 'BIOMD0000001043']\n",
      "Number of Collected IDs : 68\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T19:43:08.839366Z",
     "start_time": "2025-01-28T19:43:08.698425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "BioServices Approach\n",
    "\"\"\"\n",
    "\n",
    "s = BioModels()\n",
    "\n",
    "def get_filtered_models(query: str) -> list:\n",
    "    offset = 0\n",
    "    num_results = 10\n",
    "    all_models = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            search_results = s.search(query, numResults=num_results, offset=offset)\n",
    "\n",
    "            if search_results.get(\"models\"):\n",
    "                models = search_results[\"models\"]\n",
    "                all_models.extend(models)\n",
    "\n",
    "                offset += num_results\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if all_models:\n",
    "            print(f\"\\nTotal models : {len(all_models)}\")\n",
    "        else:\n",
    "            print(\"No matching models.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error : {str(e)}\")\n",
    "\n",
    "    return [model['id'] for model in all_models]\n",
    "\n",
    "def get_model_metadata(model_ids: list) -> dict:\n",
    "    metadata = {}\n",
    "    for model_id in model_ids:\n",
    "        try:\n",
    "            model_data = s.get_model(model_id)\n",
    "            metadata[model_id] = model_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {model_id}: {e}\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def save_metadata_to_json(metadata: dict, filename: str):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(metadata, json_file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Metadata saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "\n",
    "def download_biomodels(directory: str, model_ids: list, num_per_download=100):\n",
    "    if num_per_download > 100:\n",
    "        raise ValueError(\"Maximum number of models that can be downloaded at a time is 100.\")\n",
    "\n",
    "    total_models = len(model_ids)\n",
    "    if total_models == 0:\n",
    "        raise ValueError(\"Error : model_ids list empty.\")\n",
    "\n",
    "    num_downloads = (total_models // num_per_download) + (1 if total_models % num_per_download > 0 else 0)\n",
    "    filenames = []\n",
    "\n",
    "    for download_number in range(num_downloads):\n",
    "        start = download_number * num_per_download\n",
    "        end = min(start + num_per_download, total_models)\n",
    "        batch = model_ids[start:end]\n",
    "\n",
    "        print(f\"Downloading batch {download_number + 1}: Models {start + 1} to {end}\")\n",
    "\n",
    "        fname = os.path.join(directory, f\"Biomodels_{start + 1}_to_{end}.zip\")\n",
    "        filenames.append(fname)\n",
    "\n",
    "        if os.path.isfile(fname):\n",
    "            os.remove(fname)\n",
    "\n",
    "        try:\n",
    "            s.search_download(batch, output_filename=fname)\n",
    "            print(f\"Downloaded models {start + 1} to {end} into {fname}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading batch {download_number + 1}: {str(e)}\")\n",
    "\n",
    "    final_zip = os.path.join(directory, \"biomodels_filtered.zip\")\n",
    "    with z.ZipFile(filenames[0], 'a') as z1:\n",
    "        for fname in filenames[1:]:\n",
    "            with z.ZipFile(fname, 'r') as zf:\n",
    "                for n in zf.namelist():\n",
    "                    z1.writestr(n, zf.read(n))\n",
    "\n",
    "    if not os.path.isfile(final_zip):\n",
    "        os.rename(filenames[0], final_zip)\n",
    "\n",
    "    for fname in filenames[1:]:\n",
    "        try:\n",
    "            os.remove(fname)\n",
    "        except Exception:\n",
    "            print(f\"Could not delete temporary file: {fname}\")\n",
    "\n",
    "    print(f\"All models consolidated into {final_zip}\")\n",
    "    return final_zip\n",
    "\n",
    "def bioservices_get_models():\n",
    "    query, _ = create_query()\n",
    "    try:\n",
    "        filtered_model_ids = get_filtered_models(query)\n",
    "        model_metadata = get_model_metadata(filtered_model_ids)\n",
    "        save_metadata_to_json(model_metadata, md_path / \"model_metadata.json\")\n",
    "        output_zip = download_biomodels(\n",
    "            directory=bm_sbml_path,\n",
    "            model_ids=filtered_model_ids,\n",
    "            num_per_download=100\n",
    "        )\n",
    "\n",
    "        print(f\"Models downloaded and saved in {output_zip}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n"
   ],
   "id": "9d6b65213bd46a75",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33mWARNING [bioservices.BioModels:130]: \u001B[0m \u001B[32mThe URL (https://www.ebi.ac.uk/biomodels) provided cannot be reached.\u001B[0m\n",
      "\u001B[32mINFO    [bioservices.BioModels:363]: \u001B[0m \u001B[32mInitialising BioModels service (REST)\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T19:43:40.243279Z",
     "start_time": "2025-01-28T19:43:08.846246Z"
    }
   },
   "cell_type": "code",
   "source": "bioservices_get_models()",
   "id": "437e5f9e6d0bc627",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total models : 68\n",
      "Metadata saved to /Users/guillaume.souede/PycharmProjects/immunogit/metadata/model_metadata.json\n",
      "Downloading batch 1: Models 1 to 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mINFO    [bioservices.BioModels:240]: \u001B[0m \u001B[32m/Users/guillaume.souede/PycharmProjects/immunogit/models/BioModels/SBML/Biomodels_1_to_68.zip\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded models 1 to 68 into /Users/guillaume.souede/PycharmProjects/immunogit/models/BioModels/SBML/Biomodels_1_to_68.zip\n",
      "All models consolidated into /Users/guillaume.souede/PycharmProjects/immunogit/models/BioModels/SBML/biomodels_filtered.zip\n",
      "Models downloaded and saved in /Users/guillaume.souede/PycharmProjects/immunogit/models/BioModels/SBML/biomodels_filtered.zip\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T19:44:59.019589Z",
     "start_time": "2025-01-28T19:44:59.012912Z"
    }
   },
   "cell_type": "code",
   "source": "print(md_path)",
   "id": "471d785b4e0c20a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/guillaume.souede/PycharmProjects/immunogit/metadata\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
