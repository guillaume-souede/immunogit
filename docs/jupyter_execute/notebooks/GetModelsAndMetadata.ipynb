{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51638e82aeeabfdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T19:43:03.234573Z",
     "start_time": "2025-01-28T19:43:03.231791Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 14:49:27,950 - INFO - Root path: /Users/guillaume.souede/PycharmProjects/immunogit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 14:49:27,951 - INFO - Directory structure set up successfully.\n"
     ]
    }
   ],
   "source": [
    "from common_immunogit import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4828cdf903d1f66c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T19:43:03.253055Z",
     "start_time": "2025-01-28T19:43:03.248884Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Query Creation\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_query(domain=\"biomodels\", offset=0, num_results=100):\n",
    "    \"\"\"\n",
    "    Creates a query string and URL for searching BioModels.\n",
    "\n",
    "    Args:\n",
    "        domain (str): The domain to search within. Defaults to \"biomodels\".\n",
    "        offset (int): The starting point for search results. Defaults to 0.\n",
    "        num_results (int): The number of results to retrieve. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - query (str): The formatted query string.\n",
    "            - url (str): The URL for the search query.\n",
    "    \"\"\"\n",
    "    query_parts_full = {\n",
    "        'mode': '*:*',\n",
    "        'species': 'TAXONOMY:9606',\n",
    "        'curation_status': 'curationstatus:\"Manually curated\"',\n",
    "        'formats': 'modelformat:\"SBML\"',\n",
    "        'kw': 'submitter_keywords:\"Immuno-oncology\"'\n",
    "    }\n",
    "\n",
    "    query_parts = [value for value in query_parts_full.values() if value]\n",
    "    query = \" AND \".join(query_parts)\n",
    "\n",
    "    query_for_url = query.replace(\" \", \"%20\").replace(\":\", \"%3A\").replace('\"', \"%22\")\n",
    "    url = f\"https://www.ebi.ac.uk/biomodels/search?query={query_for_url}&domain={domain}&offset={offset}&numResults={num_results}\"\n",
    "\n",
    "    return query, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6b65213bd46a75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T19:43:08.839366Z",
     "start_time": "2025-01-28T19:43:08.698425Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    [bioservices.BioModels:363]: \u001b[0m \u001b[32mInitialising BioModels service (REST)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 14:49:28,415 - INFO - Initialising BioModels service (REST)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BioServices' way to retrieve Models\n",
    "\"\"\"\n",
    "\n",
    "s = BioModels()\n",
    "\n",
    "def get_filtered_models(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve filtered models based on a query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of model IDs matching the query.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If an error occurs during the search process.\n",
    "    \"\"\"\n",
    "    offset = 0\n",
    "    num_results = 10\n",
    "    all_models = []\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            search_results = s.search(query, numResults=num_results, offset=offset)\n",
    "\n",
    "            if search_results.get(\"models\"):\n",
    "                models = search_results[\"models\"]\n",
    "                all_models.extend(models)\n",
    "\n",
    "                offset += num_results\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if all_models:\n",
    "            print(f\"\\nTotal models : {len(all_models)}\")\n",
    "        else:\n",
    "            print(\"No matching models.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error : {str(e)}\")\n",
    "\n",
    "    return [model['id'] for model in all_models]\n",
    "\n",
    "def get_model_metadata(model_ids: list) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve metadata for a list of model IDs.\n",
    "\n",
    "    Args:\n",
    "        model_ids (list): A list of model IDs.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing metadata for each model ID.\n",
    "    \"\"\"\n",
    "    metadata = {}\n",
    "    for model_id in model_ids:\n",
    "        try:\n",
    "            model_data = s.get_model(model_id)\n",
    "            metadata[model_id] = model_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {model_id}: {e}\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "def save_metadata_to_json(metadata: dict, filename: str):\n",
    "    \"\"\"\n",
    "    Save metadata to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        metadata (dict): The metadata dictionary to save.\n",
    "        filename (str): The path to the JSON file.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during the file writing process.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(metadata, json_file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Metadata saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")\n",
    "\n",
    "def download_biomodels(directory: str, model_ids: list, num_per_download=100):\n",
    "    \"\"\"\n",
    "    Download BioModels in batches and consolidate them into a single ZIP file.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The directory to save the downloaded files.\n",
    "        model_ids (list): A list of model IDs to download.\n",
    "        num_per_download (int): The number of models to download per batch (max 100).\n",
    "\n",
    "    Returns:\n",
    "        str: The path to the consolidated ZIP file.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the model_ids list is empty or num_per_download exceeds 100.\n",
    "    \"\"\"\n",
    "    if num_per_download > 100:\n",
    "        raise ValueError(\"Maximum number of models that can be downloaded at a time is 100.\")\n",
    "\n",
    "    total_models = len(model_ids)\n",
    "    if total_models == 0:\n",
    "        raise ValueError(\"Error : model_ids list empty.\")\n",
    "\n",
    "    num_downloads = (total_models // num_per_download) + (1 if total_models % num_per_download > 0 else 0)\n",
    "    filenames = []\n",
    "\n",
    "    for download_number in range(num_downloads):\n",
    "        start = download_number * num_per_download\n",
    "        end = min(start + num_per_download, total_models)\n",
    "        batch = model_ids[start:end]\n",
    "\n",
    "        print(f\"Downloading batch {download_number + 1}: Models {start + 1} to {end}\")\n",
    "\n",
    "        fname = os.path.join(directory, f\"Biomodels_{start + 1}_to_{end}.zip\")\n",
    "        filenames.append(fname)\n",
    "\n",
    "        if os.path.isfile(fname):\n",
    "            os.remove(fname)\n",
    "\n",
    "        try:\n",
    "            s.search_download(batch, output_filename=fname)\n",
    "            print(f\"Downloaded models {start + 1} to {end} into {fname}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading batch {download_number + 1}: {str(e)}\")\n",
    "\n",
    "    final_zip = os.path.join(directory, \"biomodels_filtered.zip\")\n",
    "    with z.ZipFile(filenames[0], 'a') as z1:\n",
    "        for fname in filenames[1:]:\n",
    "            with z.ZipFile(fname, 'r') as zf:\n",
    "                for n in zf.namelist():\n",
    "                    z1.writestr(n, zf.read(n))\n",
    "\n",
    "    if not os.path.isfile(final_zip):\n",
    "        os.rename(filenames[0], final_zip)\n",
    "\n",
    "    for fname in filenames[1:]:\n",
    "        try:\n",
    "            os.remove(fname)\n",
    "        except Exception:\n",
    "            print(f\"Could not delete temporary file: {fname}\")\n",
    "\n",
    "    print(f\"All models consolidated into {final_zip}\")\n",
    "    return final_zip\n",
    "\n",
    "def bioservices_get_models():\n",
    "    \"\"\"\n",
    "    Main function to retrieve, process, and download BioModels.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Creates a query to search for models.\n",
    "    2. Retrieves filtered model IDs based on the query.\n",
    "    3. Fetches metadata for the filtered models.\n",
    "    4. Saves the metadata to a JSON file.\n",
    "    5. Downloads the models in batches and consolidates them into a single ZIP file.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If any error occurs during the process.\n",
    "    \"\"\"\n",
    "    query, _ = create_query()\n",
    "    try:\n",
    "        filtered_model_ids = get_filtered_models(query)\n",
    "        model_metadata = get_model_metadata(filtered_model_ids)\n",
    "        save_metadata_to_json(model_metadata, md_path / \"model_metadata.json\")\n",
    "        output_zip = download_biomodels(\n",
    "            directory=bm_sbml_path,\n",
    "            model_ids=filtered_model_ids,\n",
    "            num_per_download=100\n",
    "        )\n",
    "\n",
    "        print(f\"Models downloaded and saved in {output_zip}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437e5f9e6d0bc627",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T19:43:40.243279Z",
     "start_time": "2025-01-28T19:43:08.846246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total models : 68\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : [Errno 2] No such file or directory: '/Users/guillaume.souede/PycharmProjects/immunogit/metadata/metadata_all.json/model_metadata.json'\n",
      "Downloading batch 1: Models 1 to 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO    [bioservices.BioModels:240]: \u001b[0m \u001b[32m/Users/guillaume.souede/PycharmProjects/immunogit/models/BioModels/SBML/Biomodels_1_to_68.zip\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 14:49:47,363 - INFO - /Users/guillaume.souede/PycharmProjects/immunogit/models/BioModels/SBML/Biomodels_1_to_68.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded models 1 to 68 into /Users/guillaume.souede/PycharmProjects/immunogit/models/BioModels/SBML/Biomodels_1_to_68.zip\n",
      "All models consolidated into /Users/guillaume.souede/PycharmProjects/immunogit/models/BioModels/SBML/biomodels_filtered.zip\n",
      "Models downloaded and saved in /Users/guillaume.souede/PycharmProjects/immunogit/models/BioModels/SBML/biomodels_filtered.zip\n"
     ]
    }
   ],
   "source": [
    "bioservices_get_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}